{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Object Detection with TensorFlow\n",
    "*Author: Abel Brown (abelb@nvidia.com)*\n",
    "<img src=\"sheep.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<p style=\"text-align: center;color:gray\"> *Image credit: Lin et al, Microsoft COCO: Common Objects in Context*  </p>\n",
    "\n",
    "\n",
    "Welcome to an Introduction to Object Detection with TensorFlow.  The ultimate goal of this lab is to get you working with state-of-the-art object detection algorithms as fast as possible while building a good foundation for future exploration. You will learn how to configure the Google [Object Detection API](https://github.com/tensorflow/models/tree/master/object_detection) in [TensorFlow](https://www.tensorflow.org/) and evaluate detection performance working with the [Common Objects in Context](http://mscoco.org/) (COCO) data set from [Microsoft](https://www.microsoft.com/en-us/research/).  The TensorFlow Object Detection API is an open source framework built on top of TensorFlow that makes it easy to construct, train and deploy object detection models.  Within the object detection API there are five different detection models based on recent advancements in deep learning:\n",
    "1. Single Shot Multibox Detector ([SSD](https://arxiv.org/abs/1512.02325)) with [MobileNets](https://arxiv.org/abs/1704.04861)\n",
    "2. SSD with [Inception v2](https://arxiv.org/abs/1512.00567)\n",
    "3. [Region-Based Fully Convolutional Networks](https://arxiv.org/abs/1605.06409) (R-FCN) with [Resnet](https://arxiv.org/abs/1512.03385) 101\n",
    "4. [Faster RCNN](https://arxiv.org/abs/1506.01497) with Resnet 101\n",
    "5. Faster RCNN with [Inception Resnet v2](https://arxiv.org/abs/1602.07261)\n",
    "\n",
    "All these models are pre-trained using the rich COCO dataset of more than 80,000 annotated images.  The API also includes [scripts](https://github.com/tensorflow/models/blob/master/object_detection/train.py) and [instructions](https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_locally.md) for creating or extending the API with your own data to create models specifically tuned for your application.\n",
    "\n",
    "With all these models comes the fundemental engineering trade-off of accuracy versus performance.  The lightweight SSD with MobileNets model offer fast inference times around 40 milliseconds per frame (25 fps) but is considerably less accurate than the Faster RCNN with Inception Resnet v2 having an inference time of about 47 seconds per image.  Therefore, it is important to think carefully about the time and accuracy constraints of your particular problem (real-time, mobile, embedded, off-line, cluster, etc) and choose the appropriate network/model accordingly.  See [here](https://arxiv.org/abs/1611.10012) for further discussion regarding the speed and accuracy trade-offs in modern convolutional object detectors. \n",
    "\n",
    "Let's get started ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import PIL\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import zipfile\n",
    "import tarfile\n",
    "import operator\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import six.moves.urllib as urllib\n",
    "\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import PIL.ImageFont as ImageFont\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setup\n",
    "- COCO annotations\n",
    "- TensorFlow API from GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need are the COCO image annotations which provide human readable labels for the numeric detection identifiers produce by the models. For example, a \"1\" means \"person\", \"2\" means \"car\" and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# get the coco annotations if needed ... \n",
    "\n",
    "# make coco dir if not exist already\n",
    "[ ! -d coco ] && mkdir coco\n",
    "\n",
    "# move into the coco dir\n",
    "cd coco\n",
    "\n",
    "# define URL for the COCO annotations \n",
    "URL=\"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "\n",
    "# download COCO annotations if not exist\n",
    "[ ! -f annotations/instances_train2017.json ] && wget --quiet ${URL} && unzip annotations_trainval2017.zip\n",
    "\n",
    "# we're done here \n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define a relative path to the coco annotations JSON definitions \n",
    "json_annotations_file_path = os.path.join('coco','annotations','instances_train2017.json')\n",
    "\n",
    "# import the coco annotation information (big file, takes a few seconds)\n",
    "with open(json_annotations_file_path) as dat: data = json.load(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lets build an dictionary that converts/maps category numeric IDs to human readable strings\n",
    "# we will use this mapping to translate the numeric network output to something we humans can interperet\n",
    "\n",
    "# init empty dictionary for mapping\n",
    "class_id_index = dict()\n",
    "\n",
    "# iterate over categories defined in the coco annotations metadata\n",
    "for category in data['categories']: \n",
    "    \n",
    "    # get the category identifier\n",
    "    category_id = category['id']\n",
    "    \n",
    "    # get the name of the category\n",
    "    category_name= category['name']\n",
    "    \n",
    "    # get the super category\n",
    "    supercategory = category['supercategory']\n",
    "    \n",
    "    # create mapping/entry for this category id to humany readable string\n",
    "    class_id_index[category_id] = '{}/{}'.format(supercategory, category_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check:\n",
    "Add a code cell below and have a look at a few key value pairs from the `class_id_index` dictionary.  For exmaple: what string value does class_id_index[1] return?  You can get all valid dictionary keys with `class_id_index.keys()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to clone the TensorFlow [Models repository](https://github.com/tensorflow/models) from GitHub, install Google [Proto Buffers](https://developers.google.com/protocol-buffers/), Python Image Library ([PIL](https://en.wikipedia.org/wiki/Python_Imaging_Library)), [LXML](http://lxml.de/), and compile the protobuf XML definitions for the models into Python.  See full installation instruction [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# get the Google TensorFlow Object Detection API if needed \n",
    "\n",
    "# if no models dir then just clone git repo \n",
    "[ ! -d models ] && git clone https://github.com/tensorflow/models.git\n",
    "\n",
    "# install deps: protoc, PIL, and lxml\n",
    "#apt-get install -y protobuf-compiler python-pil python-lxml\n",
    "\n",
    "# move into the TF models dir\n",
    "cd models/research\n",
    "\n",
    "# compile proto buf definitions for models\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "# fin \n",
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five pre-trained models included in the object detection API.  Lets create some model IDs and a list of all the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configuration\n",
    "- model identification\n",
    "- downloading model weights\n",
    "- loading models in TensorFlow\n",
    "- initialize session in TensorFlow for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five models currently avaliable that we can use right out of the box.  As a light weight form of version control, these models were created on June 11, 2017 and that date is encoded into the file names for the models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# there are a few models we can use out-of-the-box\n",
    "\n",
    "# create date id for when these models were last updated (June 11, 2017)\n",
    "date = '11_06_2017'\n",
    "\n",
    "# create model identifiers\n",
    "model_0 = 'ssd_mobilenet_v1_coco'\n",
    "model_1 = 'ssd_inception_v2_coco'\n",
    "model_2 = 'rfcn_resnet101_coco'\n",
    "model_3 = 'faster_rcnn_resnet101_coco'\n",
    "model_4 = 'faster_rcnn_inception_resnet_v2_atrous_coco'\n",
    "\n",
    "# throw everything together into a list of model ids\n",
    "list_of_models = [model_0, model_1, model_2, model_3, model_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API models are quite large and so they can not be checked into a typical GIT repository (see [here](https://help.github.com/articles/what-is-my-disk-quota/)).  Therefore, we need to manually download the model weights for each pre-trained network.  Since the models are quite large, we do not want to download them more than once.  It is important to check if the files exist locally first before attempting to download the weights.  Notice that for each model we download the associated serialized protobuf object `frozen_inference_graph.pb`.  You can think of this serialized network as platform agnostic representation of the model and weights.  When the model is inflated or deserialized, the local Proto Buffer implementation will sort out any platform dependent issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this is the url where we download models for the Google Object Detection API\n",
    "download_url = 'http://download.tensorflow.org/models/object_detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over model ids and download+decompress if needed\n",
    "\n",
    "# contenet root for object detection assets\n",
    "rootdir = os.path.join('models','research','object_detection')\n",
    "\n",
    "# acquire the models from the internets\n",
    "for model in list_of_models:\n",
    "    \n",
    "    # create identifier for model\n",
    "    model_id = model + '_'+ date\n",
    "    \n",
    "    # create the full name of the model file\n",
    "    file_name = model_id + '.tar.gz'\n",
    "    \n",
    "    # create path to the model file\n",
    "    file_path = os.path.join(rootdir, file_name)\n",
    "    \n",
    "    # this is the file we ultimately want for each model\n",
    "    net_proto_buf_file = os.path.join(rootdir,model_id,'frozen_inference_graph.pb')\n",
    "    \n",
    "    # check for proto buf network definition file first\n",
    "    if os.path.isfile(net_proto_buf_file):\n",
    "        \n",
    "        # compute the file size\n",
    "        fsize = os.path.getsize(net_proto_buf_file)\n",
    "        \n",
    "        # confort signal\n",
    "        print 'model already exists: ({:7.2f} MB) {}'.format(fsize/1024/1024,model)\n",
    "        \n",
    "        # move to the next model\n",
    "        continue\n",
    "    \n",
    "    # check if the file already exists ...\n",
    "    if os.path.isfile(file_path):\n",
    "        \n",
    "        # compute the file size\n",
    "        fsize = os.path.getsize(file_path)\n",
    "        \n",
    "        # comfort signal for humans\n",
    "        print 'model already exists: ({:7.2f} MB) {}'.format(fsize/1024/1024,model)\n",
    "        \n",
    "        # skip download etc b/c file already exists\n",
    "        continue\n",
    "        \n",
    "    # comfort signal\n",
    "    print 'acquiring model {0}'.format(model)\n",
    "    \n",
    "    # construct the full url for this file\n",
    "    url = download_url + file_name\n",
    "    \n",
    "    # init URL request\n",
    "    opener = urllib.request.URLopener()\n",
    "    \n",
    "    # retreive the model file from the url\n",
    "    opener.retrieve(url, file_path)\n",
    "    \n",
    "    # open the tar file for extract contents\n",
    "    tar_file = tarfile.open(file_path)\n",
    "    \n",
    "    # for each file in the archive ... \n",
    "    for content in tar_file.getmembers():\n",
    "        \n",
    "        # strip out just file name\n",
    "        fname = os.path.basename(content.name)\n",
    "        \n",
    "        # if this is a inference graph ...\n",
    "        if 'frozen_inference_graph.pb' in fname:\n",
    "            \n",
    "            # compute the file size\n",
    "            fsize = os.path.getsize(file_path)\n",
    "            \n",
    "            # another comfort signal ...\n",
    "            print 'unpacking model {0}: {1:7.2f} MB'.format(model,fsize/1024/1024)\n",
    "            \n",
    "            # then unpack the file\n",
    "            tar_file.extract(content, rootdir)\n",
    "\n",
    "# display completion signal\n",
    "print 'acquiring models complete'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the models, lets select a model to use and load that model into TensorFlow.  Loading pre-trained graphs into TensorFlow has imporoved over time and is facilitated by the [metagraph](https://www.tensorflow.org/programmers_guide/meta_graph) interface.  As TensorFlow is rapidly evolving, it is always good to periodically check the docs regarding [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph), [`tf.GraphDef`](https://www.tensorflow.org/api_docs/python/tf/GraphDef), [`tf.gfile`](https://www.tensorflow.org/api_docs/python/tf/gfile), and [`tf.import_graph_def`](https://www.tensorflow.org/api_docs/python/tf/import_graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select one of the models to use\n",
    "model_choice = 0;\n",
    "\n",
    "# create model id for one of the models in the list\n",
    "model_id = list_of_models[model_choice]+'_'+date;\n",
    "\n",
    "# create relateive file path for the checkpoint file associated with this \n",
    "file_to_load = os.path.join(rootdir,model_id,'frozen_inference_graph.pb')\n",
    "\n",
    "# init empty graph\n",
    "detection_graph = tf.Graph()\n",
    "\n",
    "# scope\n",
    "with detection_graph.as_default():\n",
    "\n",
    "    # init graph definition to load protobuf\n",
    "    od_graph_def = tf.GraphDef()\n",
    "\n",
    "    # open the protobuf file\n",
    "    with tf.gfile.GFile(file_to_load, 'rb') as fid:\n",
    "\n",
    "        # read in all the contents as string\n",
    "        serialized_graph = fid.read()\n",
    "\n",
    "        # parse the string ... \n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "\n",
    "        # finally, import graph \n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "        \n",
    "# comfort signal\n",
    "print 'loaded model: {}'.format(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been loaded we can create a [`tf.Session`](https://www.tensorflow.org/api_docs/python/tf/Session) for the model (i.e. the `graph` we just loaded).  Notice that a session is created and left open so that we only pay the initialization cost once rather than setup a new session for each image.  Therefore, if we swich models it is important to close any existing session first.  Yes, it is possible to have multiple sessions active at once but let's keep it simple for now with a single session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# be sure to clean up existing session if exists\n",
    "try: sess.close() \n",
    "except: pass\n",
    "\n",
    "# initialize a new tensorflow session associated with loaded model\n",
    "sess = tf.Session(graph=detection_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, at this point we have COCO annotations, the object detection API configured, and a TensorFlow model loaded with an active session.  The only thing we need now is an image to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preparing Network Input\n",
    "- getting images from the internet\n",
    "- working with PIL images\n",
    "- preparing input tensors (Numpy arrays) for the detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper/wrapper function to display PIL image in jupyter\n",
    "def show_image(pil_image,sz=None):\n",
    "    \n",
    "    # check default image size\n",
    "    if sz is None: sz = (9,6)\n",
    "    \n",
    "    # create canvas of particular size (in inches)\n",
    "    plt.figure(figsize=sz); \n",
    "    \n",
    "    # prefer axis off for images\n",
    "    plt.axis('off'); \n",
    "    \n",
    "    # show the image\n",
    "    plt.imshow(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# interesting image from the internet\n",
    "url = 'http://www.aidatours.net/yonetim/uploads/aida-tours-lady-gaga-s-artrave-the-artpop-ball-627868-lady-gaga-09.05.2014.jpg'\n",
    "\n",
    "# open the url for reading\n",
    "fd = urllib.request.urlopen(url)\n",
    "\n",
    "# read the bytes \n",
    "image_bytes = io.BytesIO(fd.read())\n",
    "\n",
    "# init PIL image with bytes from URL\n",
    "pil_image = PIL.Image.open(image_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the image is too big Jupyter will have trouble displaying the image and the network will resize the image anyway in it's internal pre-processing pipeline before inference. So, lets resize the image, if needed, to make sure it's easy to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the size of the image\n",
    "(image_width, image_height) = pil_image.size\n",
    "\n",
    "# big images difficult in jupyter notebooks ...\n",
    "if image_width > 1024 or image_height > 1024:\n",
    "    \n",
    "    # figure out the base width for image\n",
    "    basewidth = min(image_width,1024)\n",
    "    \n",
    "    # what percentage is this a reduction\n",
    "    wpercent = (basewidth/float(image_width))\n",
    "    \n",
    "    # figure out size for scaled height\n",
    "    hsize = int((float(image_height)*float(wpercent)))\n",
    "    \n",
    "    # do resize\n",
    "    pil_image = pil_image.resize((basewidth,hsize), PIL.Image.ANTIALIAS)\n",
    "    \n",
    "    # update the size of the image\n",
    "    (image_width, image_height) = pil_image.size\n",
    "    \n",
    "# blab about the final image dimensions\n",
    "print 'image dimensions: {} pixels'.format((image_width,image_height))\n",
    "\n",
    "# show the image\n",
    "show_image(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models do not know anything about \"JPEG\" or \"PNG\" etc.  The API models only take as input arrays or matrices of numbers. Therefore, we must convert the PIL image into an arrays of pixel values. Furthremore, the model expects a 4D matrix where a single RGB color image has dimensions (1,H,W,C) where H is the number of pixels in the vertical direction, W is the number of pixels in the horizontal, and C is the number of color \"channels\" usually 3 for RGB.  Although sometimes depth field (from LiDAR etc) is included so C would be 4 in that case.  These types of images are refered to as \"[RGB-D](http://rgbd.cs.princeton.edu/)\" If we were working with video (i.e. a sequence of N imag frames) or a pair of stero images etc then the generic size of the 4D input matrix would be (N,H,W,C).  Just as an example, suppose the network input was 1-second of color video at 30 frames per second (fps) with 640 by 480 resolution.  Then the input tensor would have dimensions (30,480,640,3).  If the network input was a pair of 1024x1024 grayscale stereo images then the input tensor would have dimensions (2,1024,1024,1) where the final C dimension is 1 since there is only a single grayscale pixel value between 0 and 1 versus a color pixel value defined by three numbers R, G, and B.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# specify new dimensions for image (note w,h swap)\n",
    "new_image_dims = (image_height,image_width,3)\n",
    "\n",
    "# now, convert PIL image into NumPy array\n",
    "image_np = np.array(pil_image.getdata())\n",
    "\n",
    "# reshape the array \n",
    "image_np = image_np.reshape(new_image_dims).astype(np.uint8)\n",
    "\n",
    "# convert the array to unsigned 8-bit interger type\n",
    "image_np = image_np.astype(np.uint8)\n",
    "\n",
    "# add 4th dimension\n",
    "image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "# blab about the final array size \n",
    "print('np array dimensions: {}'.format(image_np.shape))\n",
    "\n",
    "# blab about the expanded tensor size\n",
    "print('image array expanded dimensions: {}'.format(image_np_expanded.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model Inference\n",
    "- creating the appropriate scope to reuse our session\n",
    "- creating references to tensors within the TensorFlow model\n",
    "- running the session with prepared tensor\n",
    "- understanding timing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an image we are ready to apply our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can use the session we created above as default so we do not have to pay init cost every invocation\n",
    "with detection_graph.as_default():\n",
    "  with sess.as_default():\n",
    "    \n",
    "    # a reference or handle to the \"input\" tensor for the graph\n",
    "    image_tensor_TF   = sess.graph.get_tensor_by_name('image_tensor:0'     )\n",
    "    \n",
    "    # references or handles to the the \"output\" tensors in the graph\n",
    "    boxes_TF          = sess.graph.get_tensor_by_name('detection_boxes:0'  )\n",
    "    scores_TF         = sess.graph.get_tensor_by_name('detection_scores:0' )\n",
    "    classes_TF        = sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections_TF = sess.graph.get_tensor_by_name('num_detections:0'   )\n",
    "    \n",
    "    # we need to tell the sesion to use our image tensor as input \n",
    "    # to do this we create a dictionary mapping of input tensors references to our numpy array\n",
    "    input_mapping = {image_tensor_TF: image_np_expanded}\n",
    "    \n",
    "    # we next need to tell TF about which tensors to collect for us once \n",
    "    # the input tensor we've specified has been pushed thorugh the network graph\n",
    "    # so, lets just create a list of tensors we'd like to get back after invocation\n",
    "    fetches = [boxes_TF, scores_TF, classes_TF, num_detections_TF]\n",
    "    \n",
    "    # run session and colect the outputs \n",
    "    (boxes, scores, classes, num_detections) = sess.run(fetches,feed_dict=input_mapping)\n",
    "    \n",
    "    # in human words this says, please invoke the model associated with the session \"sess\"\n",
    "    # using python variable \"image_np_expanded\" as the starting value for the \"image_tensor\"\n",
    "    # Once the compute graph has completed, go fetch the values for aforementioned tensors\n",
    "    # and map them to python variables \"boxes\", \"socres\", \"classes\", and \"num_detections\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check:\n",
    "Add the **`%%time`** cell-instruction to the to the cell above to time the model invocation/inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model inference time will depend on which model has been selected.  Also, the platform running the model will have a significant impact on the timing as well. Typically, DLI labs use a cloud service such as AWS which is a virtualized GPU resource which incurres some latency overhead.  Finally, these models come packages with an image pre-processing pipeline which performs all kinds of manipulations to the input such as resizing, mean subtraction, and on and on.  Notice, that our basic image resizing and tensor preparations have already consumed a few hundred milliseconds and we pay for more pre-processing upon model invocation!  For minimum latency, you will need to engineer the pre-processing pipeline for your particular application.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Understanding Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models for the TensorFlow Object Detection API provide four outputs: \n",
    "1. boxes - detection bounding boxes\n",
    "2. scores - sorted model confidence between 0 and 1 for the detection (bbox and classifications)\n",
    "3. classes - each detection is classified into a particular object category such as car, traffic sign, person, etc\n",
    "4. num_detections - Most of the models provide a static 100 detections every time, Faster R-CNN will output 300\n",
    "\n",
    "Notice, that detections from the models are provided in order of maximum model confidence. That is, desending order of score. So, therefore the first detection boxes[0,1] is the most confident detection from the network and so on.  You might wonder what to do with 100 detections if there are only 6 objects in the scene!  The standard practice is to discard detections under a particular threshold value (usually score of 0.5 which is 50% confidence).  Notice that the network might produce multiple detections for the same object so it is not impossible to get 13 valid detections for 8 objects and so on.  For now, this is just food for thought.  We will dive more into these details later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# each detection is associated with a bounding box encapsulating the area of interest\n",
    "# depending on which model you use, you will get 100 or 300 bounding boxes returned\n",
    "# these bboxes are defined as [ymin, xmin, ymax, xmax]\n",
    "boxes[0,0:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# each detection is associated with a confidence between 0 and 1\n",
    "# notice that the networks return detections sorted by confidence\n",
    "# that is first detection has highest score and so on\n",
    "scores[0,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each detection is associated with a numerical category id.  We will use coco annotatios definitions to decode\n",
    "classes[0,0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Working With Model Output\n",
    "- manipulating/transforming bbox values\n",
    "- using PIL to display bboxes for detections\n",
    "- creating human readable labels for detections\n",
    "- using PIL to annotate bboxes with labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the detection content from the network for the image.  What we would like to do now is to start annotating the image with detection information.  We can choose any detection we would like between `0` and `num_detections-1`.  Once we choose the detection we'd like to annotate, we need to get the associated bounding box coordinates, detection confidence, and human label.  With these data we add the bounding box rectatangle to the image and create a compact label to attach to the bbox to complete the annotation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define the detection index to show\n",
    "dix = 0;\n",
    "\n",
    "# get the bound box for this detection\n",
    "bbox = boxes[0,dix,:]\n",
    "\n",
    "# format is: ymin, xmin, ymax, xmax\n",
    "(ymin,xmin,ymax,xmax) = bbox\n",
    "\n",
    "# convert normalized coordinates to absolute\n",
    "xmin *= image_width ; xmax *= image_width; \n",
    "ymin *= image_height; ymax *= image_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to notice that we have to normalize the bbox coordinates before we can add them to the image.  The network produces normalized coordinates between 0 and 1.  This allows the bboxes to map back to any size image regardless of whatever internal image size the network uses.  Conversion from normalized [0,1] coordinates back into absolute pixel dimensions is quite simple -- just multiply x-coordinates by the original image width and the y-coordinates by the number of pixels in the image vertical.  There is nothing complicated about this, just a bit of book keeping we have to keep track of.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pil image coordinates have origin at upper left corner -- assign for readability\n",
    "(left, right, top, bottom) = (xmin, xmax, ymin, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize a drawing handel associated with our image\n",
    "pil_img_draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "# draw lines between bbox points counter-clockwise order\n",
    "pil_img_draw.line(\n",
    "      [\n",
    "          (left , top   ), # starting point\n",
    "          (left , bottom), # left edge of bbox\n",
    "          (right, bottom), # bottom edge of bbox\n",
    "          (right, top   ), # right edge of bbox\n",
    "          (left , top   )  # top edge of bbox \n",
    "      ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show the image with bounding box\n",
    "show_image(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how confident the network is for this detection and figure out the object category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the score/confidence for this detection (between zero and one)\n",
    "detection_confidence = scores[0,dix]\n",
    "\n",
    "# get the associated class for this detection\n",
    "category_id = int(classes[0,dix])\n",
    "\n",
    "# get the label for this category identifier using the index built from coco annotations file in beginning \n",
    "# that is, convert category ID from the network to human readable label for this detection\n",
    "label = class_id_index[category_id]\n",
    "\n",
    "# make a statement ...\n",
    "print 'The network is {:.2f} % confident that this is a {:s}'.format(detection_confidence*100,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a tag to add to the bounding box to form a more complete image annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets load the default system font\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "# create formated string for bbox label\n",
    "bbox_label = '{0}: {1:.2f}'.format(label,scores[0,dix]*100)\n",
    "\n",
    "# figure out the width and height of the detection label for to add to image\n",
    "text_width, text_height = font.getsize(bbox_label)\n",
    "\n",
    "# comfort signal to give us some feedback\n",
    "print 'pixel size of bbox label is {} pixels wide by {} pixels tall'.format(text_width,text_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the bbox label and we know the text size in units of pixels, we can just create another rectangle anchored at the top left corner of the bounding box, fill the rectangle with color background, and add text to the image at the specific location inside that rectangle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# draw a solid rectangle at top left corner of the bbox to host the detection label\n",
    "\n",
    "# we want the rectable to be just a bit bigger than the actual text (note: font size dependent)\n",
    "padding = np.ceil(0.05 * text_height)\n",
    "\n",
    "# upper left point of text box (just above the upper left point of the bounding box)\n",
    "ulp = (left, top - (text_height + 2 * padding))\n",
    "\n",
    "# lower right point of the text rectangle (along the top of the bbox)\n",
    "lrp = (left+text_width, top)\n",
    "\n",
    "# draw the rectangle on the PIL image anchored to bbox\n",
    "pil_img_draw.rectangle([ulp,lrp],fill='White')\n",
    "\n",
    "# finally we need to add the actual text label on to the image\n",
    "pil_img_draw.text(\n",
    "        (left + padding, top - (text_height + padding)),\n",
    "        bbox_label,\n",
    "        fill='black',\n",
    "        font=font)\n",
    "\n",
    "# show the PIL image\n",
    "show_image(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, notice that there is nothing particularly complicated about adding the additional rectangle and text.  Indeed, this is a bit of busy work to calculate the proper location and sizes. However, once coded to your preference and liking, you can forget about the details and use those brain cycles for more important things like evaluating detector performace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check:\n",
    "Repeat this section for the next/another detection from the network.  That is, update the detection index, `dix`, and work again through the cells to add bounding box and detection label.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Building Your Intuition\n",
    "Now that we have completed a single image, go back through and try some different images that are relevant to your own work.  If you want to have some fun or need some inspiration, here are a few image suggestions from round the internet (mostely from Wikimedia) [*right click img link and copy URL*].\n",
    "\n",
    "| Content       |  Link             |\n",
    "|:------------- |:-------------:|\n",
    "|Lenna |[img](https://upload.wikimedia.org/wikipedia/en/2/24/Lenna.png)|\n",
    "| Monterey Bay Aquarium | [img](https://visitmontereybay.files.wordpress.com/2015/08/capture1.png)|\n",
    "| Abbey Road | [img](https://upload.wikimedia.org/wikipedia/en/4/42/Beatles_-_Abbey_Road.jpg)|\n",
    "|Sgt. Pepper|[img](https://upload.wikimedia.org/wikipedia/en/5/50/Sgt._Pepper%27s_Lonely_Hearts_Club_Band.jpg)|\n",
    "|Marvel XMen |[img](https://i.annihil.us/u/prod/marvel//universe3zx/images/e/e2/XMenInline4.jpg)|\n",
    "|Highway in Germany|[img](https://upload.wikimedia.org/wikipedia/commons/9/97/Blick_auf_A_2_bei_Rastst%C3%A4tte_Lehrter_See_%282009%29.jpg)|\n",
    "|Gridlock Canada|[img](https://upload.wikimedia.org/wikipedia/commons/5/5d/401_Gridlock.jpg)|\n",
    "|Tight Quarters Atlanta|[img](https://upload.wikimedia.org/wikipedia/commons/b/ba/Atlanta_75.85.jpg)|\n",
    "|Public Transport Korea|[img](https://upload.wikimedia.org/wikipedia/commons/4/4d/Gyeongbu_Expressway_Bus_Only_Lane.JPG)|\n",
    "\n",
    "Right-click the img link to *copy image URL*.  Also, if want to navigate to the image, ctrl-click the link to open in a new tab rather than your current Jupyter notebook browser window.\n",
    "\n",
    "An important part of this exersize is to make an initial hypothesis about where you think the network will place the next detection.  Does that network order the detections in the order you think is correct?  Does the network prefer (bias) the right or left side of the image?  Does the network process the foreground before objects in the background?  How does the network deal with groups of objects?\n",
    "\n",
    "Of course, do not hesitate to load a different model and compare network inference time and accuracy of detection results.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Credit\n",
    "For an image:\n",
    "\n",
    "1. [easy] Calculate how many detections with confidence greater than 0.5\n",
    "2. [easy-ish] Summarize the image content of the image (i.e. maybe tally each unique category)\n",
    "3. [easy-ish] Create a color index that maps object categories to particular color of bounding box\n",
    "4. [less easy] Calculate and plot the CDF of the detection scores for a few images.  What can we tell from the curves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8  Summary\n",
    "Lets just summarize the bulk computations (tedium) we performed in this section ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function to convert TF bbox rel to abs\n",
    "def tf_bbox_rel_to_abs(pil_image,bbox):\n",
    "    \n",
    "    # bbox = list(ymin,xmin,ymax,xmax) coords in [0,1]\n",
    "    \n",
    "    # get the image dimensions\n",
    "    w,h = pil_image.size\n",
    "    \n",
    "    # format is: ymin, xmin, ymax, xmax\n",
    "    (ymin,xmin,ymax,xmax) = bbox\n",
    "\n",
    "    # convert normalized coordinates to absolute\n",
    "    xmin *= w; xmax *= w\n",
    "    ymin *= h; ymax *= h\n",
    "    \n",
    "    # we're done (note potential np.array to list\n",
    "    return [ymin,xmin,ymax,xmax]\n",
    "\n",
    "\n",
    "# function to provide image and tensor for url \n",
    "def image_for_url(url):\n",
    "    \n",
    "    # open the url for reading\n",
    "    fd = urllib.request.urlopen(url)\n",
    "\n",
    "    # read the bytes \n",
    "    image_bytes = io.BytesIO(fd.read())\n",
    "\n",
    "    # init PIL image with bytes from URL\n",
    "    pil_image = PIL.Image.open(image_bytes)\n",
    "    \n",
    "    # get the size of the image\n",
    "    (image_width, image_height) = pil_image.size\n",
    "\n",
    "    # big images difficult in jupyter notebooks ...\n",
    "    if image_width > 1024 or image_height > 1024:\n",
    "\n",
    "        # figure out the base width for image\n",
    "        basewidth = min(image_width,1024)\n",
    "\n",
    "        # what percentage is this a reduction\n",
    "        wpercent = (basewidth/float(image_width))\n",
    "\n",
    "        # figure out size for scaled height\n",
    "        hsize = int((float(image_height)*float(wpercent)))\n",
    "\n",
    "        # do resize\n",
    "        pil_image = pil_image.resize((basewidth,hsize), PIL.Image.ANTIALIAS)\n",
    "\n",
    "        # update the size of the image\n",
    "        (image_width, image_height) = pil_image.size\n",
    "        \n",
    "    # specify new dimensions for image (note w,h swap)\n",
    "    new_image_dims = (image_height,image_width,3)\n",
    "\n",
    "    # now, convert PIL image into NumPy array\n",
    "    image_np = np.array(pil_image.getdata())\n",
    "\n",
    "    # reshape the array \n",
    "    image_np = image_np.reshape(new_image_dims).astype(np.uint8)\n",
    "\n",
    "    # convert the array to unsigned 8-bit interger type\n",
    "    image_np = image_np.astype(np.uint8)\n",
    "\n",
    "    # add 4th dimension\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    \n",
    "    # we're done \n",
    "    return pil_image,image_np_expanded,\n",
    "    \n",
    "    \n",
    "def draw_bbox(pil_image, bbox, label=None, color=None):\n",
    "\n",
    "    # pid = PIL.Image.Draw\n",
    "    \n",
    "    # keep it readable\n",
    "    (ymin,xmin,ymax,xmax) = bbox\n",
    "    \n",
    "    # pil image coordinates have origin at upper left corner -- assign for readability\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "    \n",
    "    # check the color, default if needed\n",
    "    if color is None: color = 'White'\n",
    "        \n",
    "    # init image draw context\n",
    "    pid = ImageDraw.Draw(pil_image)\n",
    "    \n",
    "    # draw lines between bbox points counter-clockwise order\n",
    "    pid.line(\n",
    "      [\n",
    "          (left , top   ), # starting point\n",
    "          (left , bottom), # left edge of bbox\n",
    "          (right, bottom), # bottom edge of bbox\n",
    "          (right, top   ), # right edge of bbox\n",
    "          (left , top   )  # top edge of bbox \n",
    "      ],\n",
    "        fill=color\n",
    "    )\n",
    "    \n",
    "    # if there is not label then we're done\n",
    "    if label is None: return\n",
    "    \n",
    "    # lets load the default system font\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    # figure out the width and height of the detection label for to add to image\n",
    "    text_width, text_height = font.getsize(label)\n",
    "    \n",
    "    # we want the rectable to be just a bit bigger than the actual text (note: font size dependent)\n",
    "    padding = np.ceil(0.05 * text_height)\n",
    "\n",
    "    # upper left point of text box (just above the upper left point of the bounding box)\n",
    "    ulp = (left, top - (text_height + 2 * padding))\n",
    "\n",
    "    # lower right point of the text rectangle (along the top of the bbox)\n",
    "    lrp = (left+text_width, top)\n",
    "\n",
    "    # draw the rectangle on the PIL image anchored to bbox\n",
    "    pid.rectangle([ulp,lrp],fill=color)\n",
    "    \n",
    "    # compute the anchor point of the text\n",
    "    text_anchor_point = (left + padding, top - (text_height + padding))\n",
    "\n",
    "    # finally we need to add the actual text label on to the image\n",
    "    pid.text(text_anchor_point, label, fill='black', font=font)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check:\n",
    "Use these functions to add all the detections with a threshold of at least 0.5 to your image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# image from internet\n",
    "url = 'https://farm5.staticflickr.com/4009/4678754542_fd42c6bbb8_b_d.jpg'\n",
    "\n",
    "# get an image from the internet\n",
    "pil_image,image_tensor = image_for_url(url)\n",
    "\n",
    "# detect stuff \n",
    "with detection_graph.as_default():\n",
    "  with sess.as_default():\n",
    "    image_tensor_TF   = sess.graph.get_tensor_by_name('image_tensor:0'     )\n",
    "    boxes_TF          = sess.graph.get_tensor_by_name('detection_boxes:0'  )\n",
    "    scores_TF         = sess.graph.get_tensor_by_name('detection_scores:0' )\n",
    "    classes_TF        = sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections_TF = sess.graph.get_tensor_by_name('num_detections:0'   )\n",
    "    \n",
    "    (boxes, scores, classes, num_detections) = sess.run(\n",
    "        [boxes_TF, scores_TF, classes_TF, num_detections_TF],\n",
    "        feed_dict={image_tensor_TF: image_tensor})\n",
    "    \n",
    "# iterate over detections\n",
    "for dix in range(num_detections):\n",
    "    \n",
    "    # if threshold too low move along\n",
    "    if scores[0,dix] < 0.5: continue\n",
    "        \n",
    "    # convert the bbox rel to abs coordinates\n",
    "    bbox = tf_bbox_rel_to_abs(pil_image, boxes[0,dix])\n",
    "    \n",
    "    # create label for this detection based on class and score\n",
    "    bbox_label = '{0}: {1:.2f}'.format(\n",
    "        class_id_index[int(classes[0,dix])],\n",
    "                            scores[0,dix]*100)\n",
    "    \n",
    "    # add the detection to the image\n",
    "    draw_bbox(pil_image,bbox,bbox_label,'White')\n",
    "    \n",
    "# show the annotated result\n",
    "show_image(pil_image,(12,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check\n",
    "Try again with the lower resolution ([medium](https://farm5.staticflickr.com/4009/4678754542_fd42c6bbb8_z_d.jpg)) version of this image.  Do you notice any differences in the detection results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Working with COCO Dataset Annotations\n",
    "\n",
    "Now we are going to work with the [COCO dataset](http://mscoco.org/).  The idea is that we need a dataset with ground truth annotations to test our object detector(s).  We are interested in this dataset because it is large (+80,000 images) with content from 80 different object categories such as vehicles and people. Many images are well annotated having both object bounding boxes as well as pixel mask associated with each object in the scene to facilitate object segmentation also.  For more details on this dataset see [here](https://arxiv.org/abs/1405.0312).\n",
    "\n",
    "In this section we will review the basic algorithms for bounding box overlap and intersection over union so stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Manipulating COCO Annotations\n",
    "- understanding organization of COCO annotations\n",
    "- build mapping between image IDs and their associated annotations\n",
    "- calculating some statistics over the entire dataset\n",
    "- pulling images from COCO server and summarizing content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotations for the COCO dataset are provided as a big huge JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define a relative path to the coco annotations JSON definitions \n",
    "json_annotations_file_path = os.path.join('coco','annotations','instances_train2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the size of the annotations file\n",
    "fsize = os.path.getsize(json_annotations_file_path)/1024/1024\n",
    "\n",
    "# blab about it\n",
    "print 'The size of the COCO annotations file is {} MB'.format(fsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read in this file, python parses all of the json data and returns a the whole thing as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we already did this at the beginging but do it again if you need/want to\n",
    "# import the coco annotation information (big file, takes a few seconds) \n",
    "with open(json_annotations_file_path) as dat: data = json.load(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make note about the type return from json.load()\n",
    "print 'The data type returned from json.load() is of type {:s}'.format(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start to get a feel for what is inside of this thing lets just print the keys out to have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in data.keys():\n",
    "    print k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query any of the keys to get additional content.  For example, we used the `categories` key at the begining of this lab to create a lookup table for the network category IDs so that we could translate the numerical object labels from the detector.  Likewise, we will now use the `annotations` key to build a mapping between image IDs and their associated annotations (which includes bounding boxes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check\n",
    "Have a look at the `info` and `licenses` content for the annotations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the image content here.  We see it is a list of dictionaries.  That is a dictionary for each image entry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what is the data type of the images content\n",
    "type(data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hm, how many entries are there (hint: many)\n",
    "len(data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ok, well what is the type of each image in the list?\n",
    "type(data['images'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lets have a look at an image entry\n",
    "data['images'][2819]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is that the images content for each image includes things like a unique identifier, image dimensions (width, height), capture date, and various resource locators (URLs). Finally, we note that there are annotations for 82783 images in this metadata file. \n",
    "\n",
    "Lets do the same exercise for the `annotations` content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# what is the data type of the images content\n",
    "type(data['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many individual annotations do we have here ?\n",
    "len(data['annotations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check:\n",
    "Have a look at a randomly selected annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things you might notice is that there are a huge number of annotations here -- more than 600,000 of them.  Each annotation is tagged with a unique image identifier that associates it with an image. However, these annotations are in random order so we need to be able to pick out all the annotations from this big collection for a particular image of interest. Finally, hopefully you noticed that every annotation defines a bounding box (`bbox`) and most have associated `segmentation` mask defined as ordered verticies of a polygon with the format (x,y,x,y,...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finall, we have enough information about the layout of this metadata to build the mapping between unique image ids and annotations.  The basic idea is that for each image id, we create a bucket.  Then we simply iterate over the annotations and based on the annotation `image_id` we just put the annotation is the bucket with the same ID.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build an index that maps images to there associated COCO annotation definitions\n",
    "\n",
    "# init empty dictionary for annotation index\n",
    "annotation_index = dict()\n",
    "\n",
    "# iterate over annotations and consolodate image IDs\n",
    "for annotation in data['annotations']:\n",
    "    \n",
    "    # get the associated image id for this annotation\n",
    "    img_id = annotation['image_id']\n",
    "    \n",
    "    # do we know about this image id yet?\n",
    "    if not img_id in annotation_index:\n",
    "        \n",
    "        # if not, initialize index entry for image id (i.e. a bucket)\n",
    "        annotation_index[img_id] = list()\n",
    "        \n",
    "    # add the annotation to the associated image (bucket)\n",
    "    annotation_index[img_id].append(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new index we can now compute some interesting statistics on the COCO dataset.  For example, we can compute the histogram of *annotations per image* over the dataset.  This is just a few lines of code and quite informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute the number of annotations per image\n",
    "annotations_per_image = [len(v) for v in annotation_index.values()]\n",
    "\n",
    "# the histogram of the number of annotations per image\n",
    "plt.hist(annotations_per_image,bins=range(0,70,5),edgecolor='black')\n",
    "\n",
    "# configure the histogram\n",
    "plt.xlabel('annotation count')\n",
    "plt.ylabel('number of images')\n",
    "plt.axis([0, 60, 0, 60000])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# compute some interesting statistics\n",
    "print \"mean: {0:0.0f}, max: {1}\".format(np.mean(annotations_per_image),np.max(annotations_per_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the exponential decay of the bin counts. Using this kind of information, you might be interested in which images have 20 or more annotations for a particular detector stress test etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets grab a random COCO image to work with.  Recall in the previous section we could simply grab an image if we had it's associated URL -- and all the coco image dictionaries have a `coco_url` defined.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get a random coco image definition\n",
    "\n",
    "# figure out number of images defined\n",
    "num_coco_images = len(data['images'])\n",
    "\n",
    "# calculate a random index\n",
    "rix = random.choice(range(num_coco_images))\n",
    "\n",
    "# grab the json node for this image\n",
    "image_metadata = data['images'][rix]\n",
    "\n",
    "# the COCO id for this image\n",
    "image_id = image_metadata['id']\n",
    "\n",
    "# get the URL for this image\n",
    "url = image_metadata['coco_url']\n",
    "\n",
    "# blab about it\n",
    "print 'we can get the image for image {} at {}'.format(image_id, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pull down the image from the coco server\n",
    "pil_image,image_tensor =  image_for_url(url)\n",
    "\n",
    "# show the image\n",
    "show_image(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get the associated annotations for this images and summarize the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets now get the coco annotations for this image\n",
    "image_annotations = annotation_index[image_id]\n",
    "\n",
    "# count the number of annotations for this image\n",
    "print 'There are {} official coco annotations for image {}'.format(len(image_annotations),image_id)\n",
    "\n",
    "# init empty tally index\n",
    "coco_annotation_count = collections.Counter()\n",
    "\n",
    "# generate tally for each object/category type\n",
    "for obj in image_annotations: coco_annotation_count[obj['category_id']] += 1\n",
    "    \n",
    "# sort the tallys\n",
    "coco_annotation_tallys = sorted(coco_annotation_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# print the categories\n",
    "for tally in coco_annotation_tallys:\n",
    "    print '{:2d} coco annotation of category {}'.format(tally[1],class_id_index[tally[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to work through the summary and try to identify the image content.  For some images it can be a little challenging to find all the annotated objects.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check:\n",
    "Repeat this exersize a few more times to pull down some COCO images and summarize content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Credit:\n",
    "1. [easy-ish] build a map of categories to image IDs.  This makes it easy to find images with particular content of interest (say, more than 5 cars and at least 3 people and 1 bycicle)\n",
    "2. [easy-ish] for each category, compute number of instances over all images.  How well is your category of interest represented in the COCO dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Working with COCO Annotations\n",
    "- formating COCO bounding box definitions\n",
    "- adding COCO annotations to images\n",
    "- figuring out which annotations are associated with a particular detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the annotations for an image we would like to be able to visualize them overlaid on the image.  We did exactly this in the previous section where we visualized the bounding boxes returned from the TensorFlow networks.  Lets just do the same thing here with the COCO annotation bounding boxes.  This only catch is that the COCO annotation bboxes are in a different format than bboxes from the detector.  Recall that the TF detector bboxes have format `(ymin,xmin,ymax,xmax)` normalized while the COCO bboxes have the format `(xmin,ymin,width,height)` absolute.  Therefore, we simply need to compute (ymax,xmax) the COCO boxes to visualize them with the same codes we used above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a simple function to convert COCO bbox to TF (ymin,xmin,ymax,xmax) format\n",
    "\n",
    "def coco_bbox_format_to_tf(bbox):\n",
    "    \n",
    "    #bbox = list(x,y,w,h) in abs coords\n",
    "    \n",
    "    # assign for readability\n",
    "    x,y,w,h = bbox\n",
    "    \n",
    "    # compute tf coords\n",
    "    xmin = x\n",
    "    ymin = y\n",
    "    xmax = x + w\n",
    "    ymax = y + h\n",
    "    \n",
    "    # we're done\n",
    "    return [ymin,xmin,ymax,xmax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use this to add the COCO bounding boxes to our COCO image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# iterate over annotations and add associated bounding boxes to image\n",
    "for annotation in annotation_index[image_id]:\n",
    "    \n",
    "    # get the bounding box\n",
    "    bbox = annotation['bbox']\n",
    "    \n",
    "    # convert the bbox format for image display\n",
    "    bbox = coco_bbox_format_to_tf(bbox)\n",
    "    \n",
    "    # get the object category for this annotation \n",
    "    category_id = annotation['category_id']\n",
    "    \n",
    "    # use human category description as the bbox label\n",
    "    label = class_id_index[category_id]\n",
    "    \n",
    "    # finally, add the annotation bbox with object label\n",
    "    draw_bbox(pil_image,bbox,label,'Crimson')\n",
    "    \n",
    "pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now lets invoke the network object detection on this image and see what we pick up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with detection_graph.as_default():\n",
    "  with sess.as_default():\n",
    "    b   = sess.graph.get_tensor_by_name('image_tensor:0'     )\n",
    "    boxes_TF          = sess.graph.get_tensor_by_name('detection_boxes:0'  )\n",
    "    scores_TF         = sess.graph.get_tensor_by_name('detection_scores:0' )\n",
    "    classes_TF        = sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections_TF = sess.graph.get_tensor_by_name('num_detections:0'   )\n",
    "    \n",
    "    (boxes, scores, classes, num_detections) = sess.run(\n",
    "        [boxes_TF, scores_TF, classes_TF, num_detections_TF],\n",
    "        feed_dict={image_tensor_TF: image_tensor})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the detections over the annotations to have a quick look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over detections\n",
    "for dix in range(num_detections):\n",
    "    \n",
    "    # if threshold too low move along\n",
    "    if scores[0,dix] < 0.5: continue\n",
    "        \n",
    "    # convert the bbox rel to abs coordinates\n",
    "    bbox = tf_bbox_rel_to_abs(pil_image, boxes[0,dix])\n",
    "    \n",
    "    # add the detection to the image\n",
    "    draw_bbox(pil_image,bbox,None,'White')\n",
    "    \n",
    "# show result\n",
    "show_image(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check:\n",
    "Repeat this process a few more times for random COCO images to get a good feel for how detections overlap with annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Scoring Detections and Evaluating Detector Performance\n",
    "- Overlap test help with detection association ... kinda\n",
    "- Discuss how to proceed with missed detections\n",
    "- Computing intersection over union (IoU)\n",
    "- Greedy IoU scoring strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see after a few iterations of overlaying detections and annotations, matching the detections to the appropriate annotations for performance evaluation might not be so trivial.  It is possible for many annotations to overlap with a single detection.  How then to score the detection with respect to the ground truth annotations? \n",
    "\n",
    "We can easily compute bounding box overlap between detections and annotations with the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def bbox_do_overlap_tf_abs(A,B):\n",
    "    \n",
    "    # A = list(ymin,xmin,ymax,xmax)\n",
    "    # B = list(ymin,xmin,ymax,xmax)\n",
    "    \n",
    "    # just do it ... compute x,y,w,h for both boxes\n",
    "    ax = A[1]; ay = A[0]; aw = A[3]-A[1]; ah = A[2]-A[0]\n",
    "    bx = B[1]; by = B[0]; bw = B[3]-B[1]; bh = B[2]-B[0]\n",
    "    \n",
    "    # simple test and we're done\n",
    "    return (abs(ax - bx) * 2 < (aw + bw)) and (abs(ay - by) * 2 < (ah + bh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what is potentially worse is that is that for a given images how do we score missed detections?  To get a feel for this situation, check out COCO image [56664](http://mscoco.org/images/56664).  There are 43 official COCO annotations for this image but MobileNet `model_0` only produces 6 detections above the 0.5 confidence threshold.  One might (correctly) argue that this is potentially more an issue with the annotations rather than the network detector. None the less, figuring out a scoring process for particular images is quite difficult when working with generic datasets.  Furthermore, for situations like autonomous drivng, it is considerable difficult to score missed detections.  For example, a missed detection could be a vehicle on the other side of highway across the median in the opposite direction.  Does a missed detection of that object affect the the ability of the autonomous driving system?  Very difficult questions to answer in general.  This leads to a dataset-wide performance metrics where performance is evaluated per class rather than per image. Detector precision for each category is computed over the entire dataset.  Then the mean precision over all categories is computed to provide mean average precision (mAP).  \n",
    "\n",
    "The strategy often used to score detections is for each detection compute the IoU over all unmatched annotations and then select the annotation associated with the maximum IoU.  Lets give this strategy a whirl ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the intersection over union is fairly straight forward -- figure out the coordinates of the overlapping rectangle, compute the area of this intersection and compute `areaI/(areaA + areaB - areaI)`.  If the calculation of the coordinates for the intersecting rectangle look strange, just sit in a quiet place with a coffee, pen, and paper for a bit and convince yourself.  The only other \"GOTCHA\" in this calculation is \"negative area\" -- which happens only when there is definitively no bounding box overlap.  Therefore we just take the max of the calculated widths and heights w.r.t. zero to keep things physical. Otherwise, this calculation is just basic stuff: `area = w*h`, don't let the book keeping confuse you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function to compute the intersection over union of these two bounding boxes\n",
    "def bbox_IoU(A, B):\n",
    "    \n",
    "    # A = list(ymin,xmin,ymax,xmax)\n",
    "    # B = list(ymin,xmin,ymax,xmax)\n",
    "    \n",
    "    # assign for readability \n",
    "    yminA, xminA, ymaxA, xmaxA = A\n",
    "    yminB, xminB, ymaxB, xmaxB = B\n",
    "    \n",
    "    # figure out the intersecting rectangle coordinates\n",
    "    xminI = max(xminA, xminB)\n",
    "    yminI = max(yminA, yminB)\n",
    "    xmaxI = min(xmaxA, xmaxB)\n",
    "    ymaxI = min(ymaxA, ymaxB)\n",
    "    \n",
    "    # compute the width and height of the intereseting rectangle\n",
    "    wI = xmaxI - xminI\n",
    "    hI = ymaxI - yminI\n",
    " \n",
    "    # compute the area of intersection rectangle (enforce area>=0)\n",
    "    areaI = max(0,wI) * max(0,hI)\n",
    "    \n",
    "    # if intersecting area is zero, we're done (avoids IoU=0/0 also)\n",
    "    if areaI == 0: return 0\n",
    " \n",
    "    # compute areas of the input bounding boxes \n",
    "    areaA = (xmaxA - xminA) * (ymaxA - yminA)\n",
    "    areaB = (xmaxB - xminB) * (ymaxB - yminB)\n",
    " \n",
    "    # finally, compute and return the intersection over union \n",
    "    return areaI / (areaA + areaB - areaI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test this out by computing the IoU of a particular detection w.r.t. all COCO annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# init empty\n",
    "iou = list()\n",
    "\n",
    "# index of a detection\n",
    "dix = 0\n",
    "\n",
    "# get the abs bbox associated with detection\n",
    "d_bbox = tf_bbox_rel_to_abs(pil_image,boxes[0,dix])\n",
    "\n",
    "# compute IoU w.r.t. to all annotations\n",
    "for annotation in annotation_index[image_id]:\n",
    "    \n",
    "    # get the annotation bbox in tf format\n",
    "    a_bbox = coco_bbox_format_to_tf(annotation['bbox'])\n",
    "    \n",
    "    # compute iou\n",
    "    iou.append(bbox_IoU(d_bbox,a_bbox))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are likely multiple non-zero IoU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# manually (by eye) inspect the IoUs\n",
    "print iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then match this detection with the annotation having maximum IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# figure out the index of the maxium IoU annotation\n",
    "mix = np.argmax(iou)\n",
    "\n",
    "# get the associated annotation\n",
    "matching_annotation = annotation_index[image_id][mix]\n",
    "\n",
    "# get the label for the detection category\n",
    "d_label = class_id_index[int(classes[0,dix])]\n",
    "\n",
    "# get the label for the annotation category\n",
    "a_label = class_id_index[matching_annotation['category_id']]\n",
    "\n",
    "# blab about it\n",
    "print 'detection {:d} ({:s}) matching annotation {:d} ({:s}) with max(IoU) {:.2f}'.format(dix,d_label,mix,a_label,iou[mix])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn all this into a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_IoU_annotation(bbox,annotations):\n",
    "    \n",
    "    # init empty\n",
    "    iou = list()\n",
    "    \n",
    "    # compute IoU w.r.t. to all annotations\n",
    "    for ax in annotations:\n",
    "    \n",
    "        # get the annotation bbox in tf format\n",
    "        a_bbox = coco_bbox_format_to_tf(ax['bbox'])\n",
    "\n",
    "        # compute iou\n",
    "        iou.append(bbox_IoU(bbox,a_bbox))\n",
    "        \n",
    "    # index of arg max\n",
    "    ix = np.argmax(iou)\n",
    "        \n",
    "    # arg max and associated annotation\n",
    "    return ix,iou[ix],annotations[ix]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets repeat this process for all valid detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get copy of annotations list (so we can remove items safely)\n",
    "alist = copy.deepcopy(annotation_index[image_id])\n",
    "\n",
    "# init empty list of matching/max annotations\n",
    "mlist = list()\n",
    "\n",
    "# init empty list of max iou values for labeling\n",
    "ilist = list()\n",
    "\n",
    "# init empty list of index values\n",
    "ixlist = list()\n",
    "\n",
    "# loop over detections one-by-one\n",
    "for dix in range(num_detections):\n",
    "    \n",
    "    # if below threshold, move along\n",
    "    if scores[0,dix] < 0.5: continue\n",
    "    \n",
    "    # get the abs bbox associated with detection\n",
    "    d_bbox = tf_bbox_rel_to_abs(pil_image,boxes[0,dix])\n",
    "    \n",
    "    # get max IoU annotation w.r.t. to detction bounding box\n",
    "    mix, miou, max_annotation = max_IoU_annotation(d_bbox,alist) \n",
    "    \n",
    "    # keep track of results\n",
    "    mlist.append(max_annotation); ilist.append(miou); ixlist.append(mix)\n",
    "    \n",
    "    # note this allows for duplicate matches, should delete matched annotations from alist (!!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, plot the final matching results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pull down the image from the coco server\n",
    "pil_image,image_tensor =  image_for_url(url)\n",
    "\n",
    "# iterate over detections\n",
    "for dix in range(num_detections):\n",
    "    \n",
    "    # if threshold too low move along\n",
    "    if scores[0,dix] < 0.5: continue\n",
    "        \n",
    "    # convert the bbox rel to abs coordinates\n",
    "    bbox = tf_bbox_rel_to_abs(pil_image, boxes[0,dix])\n",
    "    \n",
    "    # create label for this detection based on class and score\n",
    "    d_bbox_label = '{0}: {1:.2f}'.format(\n",
    "        class_id_index[int(classes[0,dix])],\n",
    "                            scores[0,dix]*100)\n",
    "    \n",
    "    # if the max IoU is approx zero then move along\n",
    "    if ilist[dix] <= 0.01: continue\n",
    "        \n",
    "    # get the associated annotation\n",
    "    matching_annotation = mlist[dix]\n",
    "    \n",
    "    # index of max iou\n",
    "    mix = ixlist[dix]\n",
    "    \n",
    "    # value of the max IoU\n",
    "    max_iou = ilist[dix]\n",
    "\n",
    "    # get the label for the detection category\n",
    "    d_label = class_id_index[int(classes[0,dix])]\n",
    "\n",
    "    # get the label for the annotation category\n",
    "    a_label = class_id_index[matching_annotation['category_id']]\n",
    "    \n",
    "    # create format string\n",
    "    info_string = 'detection {:d} ({:s}) matching annotation {:d} ({:s}) with max(IoU) {:.2f}'\n",
    "\n",
    "    # blab about it\n",
    "    print info_string.format(dix,d_label,mix,a_label,max_iou)\n",
    "        \n",
    "    # get the bounding box for this annotation abs\n",
    "    a_bbox = coco_bbox_format_to_tf(matching_annotation['bbox'])\n",
    "    \n",
    "    # annotation bbox color \n",
    "    a_color = 'LimeGreen'\n",
    "    \n",
    "    # if categories match annotation box is green\n",
    "    if not d_label == a_label: a_color = 'Crimson'\n",
    "        \n",
    "    # finally update detection bounding box label with associated max IoU\n",
    "    d_bbox_label = d_bbox_label + '/' + '{:.2f}'.format(max_iou) \n",
    "    \n",
    "    # add max IoU matching detection \n",
    "    draw_bbox(pil_image,a_bbox,None,a_color)\n",
    "    \n",
    "    # add the detection with label to the image\n",
    "    draw_bbox(pil_image,bbox,d_bbox_label,'White')\n",
    "    \n",
    "# show the annotated result\n",
    "show_image(pil_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we still have possiblility for duplicate matching annotations.  Also, if you repeate this process for a many random COCO images, you will see that often a quite small maximum IoU value for some detection-annotation match.  Often these low max-IoU associations fail to match in object category.  It would be reasonable to add a minimum max-IoU threshold to catch these cases.  Just some food for thought.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Summary\n",
    "Congratulations for making it all the way to the end!  We've learned how to shread through the COCO annotations, pull down test images from the COCO data servers and match detections with annotations using intersection over union.  No doubt there has been a large amount of book keeping -- probably more book keeping than deep learning!  But that's the way it goes sometimes.  You should now have a essential understanding of how to utilize the Object Detection API in TensorFlow as well as the core skills to evaluate object detection performance.  Be sure to play around with all the five available models and do not hesitate to create/test your own performance metrics.  As you have probably noticed, sometimes the detectors can do weird things even though the model mAP looks strong, so be sure to test test test. \n",
    "\n",
    "Don't hesitate to reach out with questions or comments at: `abelb@nvidia.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Exercises "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
